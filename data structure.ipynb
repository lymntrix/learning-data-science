{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nBig O Notation\\nUsing not-boring math to measure code\\'s efficiency\\nThe idea behind big O notation\\n\\nBig O notation is the language we use for talking about how long an algorithm takes to run. It\\'s how we compare the efficiency of different approaches to a problem.\\n\\nIt\\'s like math except it\\'s an awesome, not-boring kind of math where you get to wave your hands through the details and just focus on what\\'s basically happening.\\n\\nWith big O notation we express the runtime in terms of—brace yourself—how quickly it grows relative to the input, as the input gets arbitrarily large.\\n\\nLet\\'s break that down:\\n\\n    how quickly the runtime grows—It\\'s hard to pin down the exact runtime of an algorithm. It depends on the speed of the processor, what else the computer is running, etc. So instead of talking about the runtime directly, we use big O notation to talk about how quickly the runtime grows.\\n    relative to the input—If we were measuring our runtime directly, we could express our speed in seconds. Since we\\'re measuring how quickly our runtime grows, we need to express our speed in terms of...something else. With Big O notation, we use the size of the input, which we call \"nnn.\" So we can say things like the runtime grows \"on the order of the size of the input\" (O(n)O(n)O(n)) or \"on the order of the square of the size of the input\" (O(n2)O(n^2)O(n2)).\\n    as the input gets arbitrarily large—Our algorithm may have steps that seem expensive when nnn is small but are eclipsed eventually by other steps as nnn gets huge. For big O analysis, we care most about the stuff that grows fastest as the input grows, because everything else is quickly eclipsed as nnn gets very large. (If you know what an asymptote is, you might see why \"big O analysis\" is sometimes called \"asymptotic analysis.\")\\n\\nIf this seems abstract so far, that\\'s because it is. Let\\'s look at some examples.\\nSome examples\\n\\n  def print_first_item(items):\\n    print items[0]\\n\\nThis function runs in O(1)O(1)O(1) time (or \"constant time\") relative to its input. The input list could be 1 item or 1,000 items, but this function would still just require one \"step.\"\\n\\n  def print_all_items(items):\\n    for item in items:\\n        print item\\n\\nThis function runs in O(n)O(n)O(n) time (or \"linear time\"), where nnn is the number of items in the list. If the list has 10 items, we have to print 10 times. If it has 1,000 items, we have to print 1,000 times.\\n\\n  def print_all_possible_ordered_pairs(items):\\n    for first_item in items:\\n        for second_item in items:\\n            print first_item, second_item\\n\\nHere we\\'re nesting two loops. If our list has nnn items, our outer loop runs nnn times and our inner loop runs nnn times for each iteration of the outer loop, giving us n2n^2n2 total prints. Thus this function runs in O(n2)O(n^2)O(n2) time (or \"quadratic time\"). If the list has 10 items, we have to print 100 times. If it has 1,000 items, we have to print 1,000,000 times.\\nN could be the actual input, or the size of the input\\n\\nBoth of these functions have O(n)O(n)O(n) runtime, even though one takes an integer as its input and the other takes a list:\\n\\n  def say_hi_n_times(n):\\n    for time in xrange(n):\\n        print \"hi\"\\n\\n\\ndef print_all_items(items):\\n    for item in items:\\n        print item\\n\\nSo sometimes nnn is an actual number that\\'s an input to our function, and other times nnn is the number of items in an input list (or an input map, or an input object, etc.).\\n\\nDrop the constants\\n\\nThis is why big O notation rules. When you\\'re calculating the big O complexity of something, you just throw out the constants. So like:\\n\\n  def print_all_items_twice(items):\\n    for item in items:\\n        print item\\n\\n    # Once more, with feeling\\n    for item in items:\\n        print item\\n\\nThis is O(2n)O(2n)O(2n), which we just call O(n)O(n)O(n).\\n\\n  def print_first_item_then_first_half_then_say_hi_100_times(items):\\n    print items[0]\\n\\n    middle_index = len(items) / 2\\n    index = 0\\n    while index < middle_index:\\n        print items[index]\\n        index += 1\\n\\n    for time in xrange(100):\\n        print \"hi\"\\n\\nThis is O(1+n/2+100)O(1 + n/2 + 100)O(1+n/2+100), which we just call O(n)O(n)O(n).\\n\\nWhy can we get away with this? Remember, for big O notation we\\'re looking at what happens as nnn gets arbitrarily large. As nnn gets really big, adding 100 or dividing by 2 has a decreasingly significant effect. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "Data Structures for Coding Interviews\n",
    "Computer science in plain English\n",
    "\n",
    "To really understand how data structures work, we're going to derive each of them from scratch. Starting with bits.\n",
    "\n",
    "Don't worry—we'll skip the convoluted academic jargon and proofs.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "    Random Access Memory\n",
    "    Binary Numbers\n",
    "    Fixed-Width Integers\n",
    "    Arrays\n",
    "    Strings\n",
    "    Pointers\n",
    "    Dynamic Arrays\n",
    "    Linked Lists\n",
    "    Hash Tables\n",
    "\n",
    "Random Access Memory (RAM)\n",
    "\n",
    "When a computer is running code, it needs to keep track of variables (numbers, strings, arrays, etc.).\n",
    "\n",
    "Variables are stored in random access memory (RAM). We sometimes call RAM \"working memory\" or just \"memory.\"\n",
    "\n",
    "RAM is not where mp3s and apps get stored. In addition to \"memory,\" your computer has storage (sometimes called \"persistent storage\" or \"disc\"). While memory is where we keep the variables our functions allocate as they crunch data for us, storage is where we keep files like mp3s, videos, Word documents, and even executable programs or apps.\n",
    "\n",
    "Memory (or RAM) is faster but has less space, while storage (or \"disc\") is slower but has more space. A modern laptop might have ~500GB of storage but only ~16GB of RAM.\n",
    "\n",
    "Think of RAM like a really tall bookcase with a lot of shelves. Like, billions of shelves.\n",
    "A column of empty RAM slots.\n",
    "\n",
    "It just keeps going down. Again, picture billions of these shelves.\n",
    "\n",
    "The shelves are numbered.\n",
    "A column of empty RAM slots with indices.\n",
    "\n",
    "We call a shelf's number its address.\n",
    "\n",
    "Each shelf holds 8 bits. A bit is a tiny electrical switch that can be turned \"on\" or \"off.\" But instead of calling it \"on\" or \"off\" we call it 1 or 0.\n",
    "A column of RAM slots filled with various bits that make up bytes.\n",
    "\n",
    "8 bits is called a byte. So each shelf has one byte (8 bits) of storage.\n",
    "\n",
    "Of course, we also have a processor that does all the real work inside our computer:\n",
    "A section of RAM connected to the computer's processor, which does most of the heavy lifting.\n",
    "\n",
    "It's connected to a memory controller. The memory controller does the actual reading and writing to and from RAM. It has a direct connection to each shelf of RAM.\n",
    "The computer's processor connected to a memory controller, which does the actual reading and writing to and from RAM.\n",
    "\n",
    "That direct connection is important. It means we can access address 0 and then immediately access address 918,873 without having to \"climb down\" our massive bookshelf of RAM.\n",
    "\n",
    "That's why we call it Random Access Memory (RAM)—we can Access the bits at any Random address in Memory right away.\n",
    "\n",
    "Spinning hard drives don't have this \"random access\" superpower, because there's no direct connection to each byte on the disc. Instead, there's a reader—called a head—that moves along the surface of a spinning storage disc (like the needle on a record player). Reading bytes that are far apart takes longer because you have to wait for the head to physically move along the disc.\n",
    "\n",
    "Even though the memory controller can jump between far-apart memory addresses quickly, programs tend to access memory that's nearby. So computers are tuned to get an extra speed boost when reading memory addresses that're close to each other. Here's how it works:\n",
    "\n",
    "The processor has a cache where it stores a copy of stuff it's recently read from RAM.\n",
    "A series of caches inside of the memory controller, where the processor stores what it has recently read from RAM.\n",
    "\n",
    "Actually, it has a series of caches. But we can picture them all lumped together as one cache like this.\n",
    "\n",
    "This cache is much faster to read from than RAM, so the processor saves time whenever it can read something from cache instead of going out to RAM.\n",
    "\n",
    "When the processor asks for the contents of a given memory address, the memory controller also sends the contents of a handful of nearby memory addresses. And the processor puts all of it in the cache.\n",
    "\n",
    "So if the processor asks for the contents of address 951, then 952, then 953, then 954...it'll go out to RAM once for that first read, and the subsequent reads will come straight from the super-fast cache.\n",
    "\n",
    "But if the processor asks to read address 951, then address 362, then address 419...then the cache won't help, and it'll have to go all the way out to RAM for each read.\n",
    "\n",
    "So reading from sequential memory addresses is faster than jumping around.\n",
    "Binary numbers\n",
    "\n",
    "Let's put those bits to use. Let's store some stuff. Starting with numbers.\n",
    "\n",
    "The number system we usually use (the one you probably learned in elementary school) is called base 10, because each digit has ten possible values (1, 2, 3, 4, 5, 6, 7, 8, 9, and 0).\n",
    "\n",
    "But computers don't have digits with ten possible values. They have bits with two possible values. So they use base 2 numbers.\n",
    "\n",
    "Base 10 is also called decimal. Base 2 is also called binary.\n",
    "\n",
    "To understand binary, let's take a closer look at how decimal numbers work. Take the number \"101\" in decimal:\n",
    "In base 10, the digits 101 represent 1 hundred, 0 tens, and 1 one.\n",
    "\n",
    "Notice we have two \"1\"s here, but they don't mean the same thing. The leftmost \"1\" means 100, and the rightmost \"1\" means 1. That's because the leftmost \"1\" is in the hundreds place, while the rightmost \"1\" is in the ones place. And the \"0\" between them is in the tens place.\n",
    "In base 10, the digits 101 represent 1 hundred, 0 tens, and 1 one.\n",
    "\n",
    "So this \"101\" in base 10 is telling us we have \"1 hundred, 0 tens, and 1 one.\"\n",
    "In base 10, the digits 101 represent 1 hundred, 0 tens, and 1 one, which add to give the value one hundred and one.\n",
    "\n",
    "Notice how the places in base 10 (ones place, tens place, hundreds place, etc.) are sequential powers of 10:\n",
    "\n",
    "    100=1 10^0=1 100=1\n",
    "    101=10 10^1=10 101=10\n",
    "    102=100 10^2=100 102=100\n",
    "    103=1000 10^3=1000 103=1000\n",
    "    etc.\n",
    "\n",
    "The places in binary (base 2) are sequential powers of 2:\n",
    "\n",
    "    20=12^0=120=1\n",
    "    21=22^1=221=2\n",
    "    22=42^2=422=4\n",
    "    23=82^3=823=8\n",
    "    etc.\n",
    "\n",
    "So let's take that same \"101\" but this time let's read it as a binary number:\n",
    "In base 2, the digits 101 represent 1 four, 0 twos, and 1 one.\n",
    "\n",
    "Reading this from right to left: we have a 1 in the ones place, a 0 in the twos place, and a 1 in the fours place. So our total is 4 + 0 + 1 which is 5.\n",
    "In base 2, the digits 101 represent 1 four, 0 twos, and 1 one, which add to give the value five.\n",
    "\n",
    "Here's how we'd count up to 12 in binary:\n",
    "Decimal \tBinary\n",
    "000 \t0000\n",
    "111 \t0001\n",
    "222 \t0010\n",
    "333 \t0011\n",
    "444 \t0100\n",
    "555 \t0101\n",
    "666 \t0110\n",
    "777 \t0111\n",
    "888 \t1000\n",
    "999 \t1001\n",
    "101010 \t1010\n",
    "111111 \t1011\n",
    "121212 \t1100\n",
    "\n",
    "So far we've been talking about unsigned integers (\"unsigned\" means non-negative, and \"integer\" means a whole number, not a fraction or decimal). Storing other numbers isn't hard though. Here's how some other numbers could be stored:\n",
    "\n",
    "Fractions: Store two numbers: the numerator and the denominator.\n",
    "\n",
    "Decimals: Also two numbers: 1) the number with the decimal point taken out, and 2) the position where the decimal point goes (how many digits over from the leftmost digit).\n",
    "\n",
    "Negative Numbers: Reserve the leftmost bit for expressing the sign of the number. 0 for positive and 1 for negative.\n",
    "\n",
    "In reality we usually do something slightly fancier for each of these. But these approaches work, and they show how we can express some complex stuff with just 1s and 0s.\n",
    "\n",
    "We've talked about base 10 and base 2...you may have also seen base 16, also called hexadecimal or hex.\n",
    "\n",
    "In hex, our possible values for each digit are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, and f. Hex numbers are often prefixed with \"0x\" or \"#\".\n",
    "\n",
    "In CSS, colors are sometimes expressed in hex. Interview Cake's signature blue color is \"#5bc0de\".\n",
    "Fixed-width integers\n",
    "\n",
    "How many different numbers can we express with 1 byte (8 bits)?\n",
    "\n",
    "28=2562^8=25628=256 different numbers. How did we know to take 282^828? ↴\n",
    "\n",
    "What happens if we have the number 255 in an 8-bit unsigned integer (1111 1111 in binary) and we add 1? The answer (256) needs a 9th bit (1 0000 0000). But we only have 8 bits!\n",
    "\n",
    "This is called an integer overflow. At best, we might just get an error. At worst, our computer might compute the correct answer but then just throw out the 9th bit, giving us zero (0000 0000) instead of 256 (1 0000 0000)! (Python actually notices that the result won't fit and automatically allocates more bits to store the larger number.)\n",
    "\n",
    "The 256 possibilities we get with 1 byte are pretty limiting. So we usually use 4 or 8 bytes (32 or 64 bits) for storing integers.\n",
    "\n",
    "    32-bit integers have 2322^{32}232 possible values—more than 4 billion\n",
    "    64-bit integers have 2642^{64}264 possible values—more than 10 billion billion (101910^{19}1019).\n",
    "\n",
    "\"How come I've never had to think about how many bits my integers are?\" Maybe you have but just didn't know it.\n",
    "\n",
    "Have you ever noticed how in some languages (like Java and C) sometimes numbers are Integers and sometimes they're Longs? The difference is the number of bits (in Java, Integers are 32 bits and Longs are 64).\n",
    "\n",
    "Ever created a table in SQL? When you specify that a column will hold integers, you have to specify how many bytes: 1 byte (tinyint), 2 bytes (smallint), 4 bytes (int), or 8 bytes (bigint).\n",
    "\n",
    "When is 32 bits not enough? When you're counting views on a viral video. YouTube famously ran into trouble when the Gangnam Style video hit over 2312^{31}231 views, forcing them to upgrade their view counts from 32-bit to 64-bit signed integers.\n",
    "\n",
    "Most integers are fixed-width or fixed-length, which means the number of bits they take up doesn't change.\n",
    "\n",
    "It's usually safe to assume an integer is fixed-width unless you're told otherwise. Variable-size numbers exist, but they're only used in special cases.\n",
    "\n",
    "If we have a 64-bit fixed-length integer, it doesn't matter if that integer is 0 or 193,457—it still takes up the same amount of space in RAM: 64 bits.\n",
    "\n",
    "Are you familiar with big O notation? It's a tool we use for talking about how much time an algorithm takes to run or how much space a data structure takes up in RAM. It's pretty simple:\n",
    "\n",
    "O(1)O(1)O(1) or constant means the time or space stays about the same even as the dataset gets bigger and bigger.\n",
    "\n",
    "O(n)O(n)O(n) or linear means the time or space grows proportionally as the dataset grows.\n",
    "\n",
    "So O(1)O(1)O(1) space is much smaller than O(n)O(n)O(n) space. And O(1)O(1)O(1) time is much faster than O(n)O(n)O(n) time.\n",
    "\n",
    "That's all you need for this piece. But if you're curious, you can read our whole big O explainer here.\n",
    "\n",
    "In big O notation, we say fixed-width integers take up constant space or O(1)O(1)O(1) space.\n",
    "\n",
    "And because they have a constant number of bits, most simple operations on fixed-width integers (addition, subtraction, multiplication, division) take constant time (O(1)O(1)O(1) time).\n",
    "\n",
    "So fixed-width integers are very space efficient and time efficient.\n",
    "\n",
    "But that efficiency comes at a cost—their values are limited. Specifically, they're limited to 2n2^n2n possibilities, where nnn is the number of bits.\n",
    "\n",
    "So there's a tradeoff. As we'll see, that's a trend in data structures—to get a nice property, we'll often have to lose something.\n",
    "Arrays\n",
    "\n",
    "Ok, so we know how to store individual numbers. Let's talk about storing several numbers.\n",
    "\n",
    "That's right, things are starting to heat up.\n",
    "\n",
    "Suppose we wanted to keep a count of how many bottles of kombucha we drink every day.\n",
    "\n",
    "Let's store each day's kombucha count in an 8-bit, fixed-width, unsigned integer. That should be plenty—we're not likely to get through more than 256 (282^828) bottles in a single day, right?\n",
    "\n",
    "And let's store the kombucha counts right next to each other in RAM, starting at memory address 0:\n",
    "A stack of RAM in which we store kombucha counts starting at index 0.\n",
    "\n",
    "Bam. That's an array. RAM is basically an array already.\n",
    "\n",
    "Just like with RAM, the elements of an array are numbered. We call that number the index of the array element (plural: indices). In this example, each array element's index is the same as its address in RAM.\n",
    "\n",
    "But that's not usually true. Suppose another program like Spotify had already stored some information at memory address 2:\n",
    "A column of 9 RAM slots representing an array. The row at index 2 is highlighted because it is being used by Spotify.\n",
    "\n",
    "We'd have to start our array below it, for example at memory address 3. So index 0 in our array would be at memory address 3, and index 1 would be at memory address 4, etc.:\n",
    "A column of 9 RAM slots representing an array. The row at index 2 is highlighted, and the rows at indices 3 through 7 are selected with a bracket.\n",
    "\n",
    "Suppose we wanted to get the kombucha count at index 4 in our array. How do we figure out what address in memory to go to? Simple math:\n",
    "\n",
    "Take the array's starting address (3), add the index we're looking for (4), and that's the address of the item we're looking for. 3 + 4 = 7. In general, for getting the nnnth item in our array:\n",
    "address of nth item in array= \\text{address of nth item in array} = address of nth item in array= address of array start+n \\text{address of array start} + n address of array start+n\n",
    "\n",
    "This works out nicely because the size of the addressed memory slots and the size of each kombucha count are both 1 byte. So a slot in our array corresponds to a slot in RAM.\n",
    "\n",
    "But that's not always the case. In fact, it's usually not the case. We usually use 64-bit integers.\n",
    "\n",
    "So how do we build an array of 64-bit (8 byte) integers on top of our 8-bit (1 byte) memory slots?\n",
    "\n",
    "We simply give each array index 8 address slots instead of 1:\n",
    "A column of RAM slots representing an array of 64-bit integers. Every 8 buckets of RAM represents one integer.\n",
    "\n",
    "So we can still use simple math to grab the start of the nthnthnth item in our array—just gotta throw in some multiplication:\n",
    "address of nth item in array= \\text{address of nth item in array} = address of nth item in array= address of array start+(n∗size of each item in bytes) \\text{address of array start} + (n * \\text{size of each item in bytes}) address of array start+(n∗size of each item in bytes)\n",
    "\n",
    "Don't worry—adding this multiplication doesn't really slow us down. Remember: addition, subtraction, multiplication, and division of fixed-width integers takes O(1)O(1)O(1) time. So all the math we're using here to get the address of the nnnth item in the array takes O(1)O(1)O(1) time.\n",
    "\n",
    "And remember how we said the memory controller has a direct connection to each slot in RAM? That means we can read the stuff at any given memory address in O(1)O(1)O(1) time.\n",
    "A memory controller connected to a section of RAM.\n",
    "\n",
    "Together, this means looking up the contents of a given array index is O(1)O(1)O(1) time. This fast lookup capability is the most important property of arrays.\n",
    "\n",
    "But the formula we used to get the address of the nnnth item in our array only works if:\n",
    "\n",
    "    Each item in the array is the same size (takes up the same number of bytes).\n",
    "    The array is uninterrupted (contiguous) in memory. There can't be any gaps in the array...like to \"skip over\" a memory slot Spotify was already using.\n",
    "\n",
    "These things make our formula for finding the nnnth item work because they make our array predictable. We can predict exactly where in memory the nnnth element of our array will be.\n",
    "\n",
    "But they also constrain what kinds of things we can put in an array. Every item has to be the same size. And if our array is going to store a lot of stuff, we'll need a bunch of uninterrupted free space in RAM. Which gets hard when most of our RAM is already occupied by other programs (like Spotify).\n",
    "\n",
    "That's the tradeoff. Arrays have fast lookups (O(1)O(1)O(1) time), but each item in the array needs to be the same size, and you need a big block of uninterrupted free memory to store the array.\n",
    "Strings\n",
    "\n",
    "Okay, let's store some words.\n",
    "\n",
    "A series of characters (letters, punctuation, etc.) is called a string.\n",
    "\n",
    "We already know one way to store a series of things—arrays. But how can an array store characters instead of numbers?\n",
    "\n",
    "Easy. Let's define a mapping between numbers and characters. Let's say \"A\" is 1 (or 0000 0001 in binary), \"B\" is 2 (or 0000 0010 in binary), etc. Bam. Now we have characters.\n",
    "\n",
    "This mapping of numbers to characters is called a character encoding. One common character encoding is \"ASCII\". Here's how the alphabet is encoded in ASCII:\n",
    "A: 01000001\n",
    "S: 01010011\n",
    "k: 01101011\n",
    "B: 01000010\n",
    "T: 01010100\n",
    "l: 01101100\n",
    "C: 01000011\n",
    "U: 01010101\n",
    "m: 01101101\n",
    "D: 01000100\n",
    "V: 01010110\n",
    "n: 01111110\n",
    "E: 01000101\n",
    "W: 01010111\n",
    "o: 01101111\n",
    "F: 01000110\n",
    "X: 01011000\n",
    "p: 01110000\n",
    "G: 01000111\n",
    "Y: 01011001\n",
    "q: 01110001\n",
    "H: 01001000\n",
    "Z: 01011010\n",
    "r: 01110010\n",
    "I: 01001001\n",
    "a: 01100001\n",
    "s: 01110011\n",
    "J: 01001010\n",
    "b: 01100010\n",
    "t: 01110100\n",
    "K: 01001011\n",
    "c: 01100011\n",
    "u: 01110101\n",
    "L: 01001100\n",
    "d: 01100100\n",
    "v: 01110110\n",
    "M: 01001101\n",
    "e: 01100101\n",
    "w: 01110111\n",
    "N: 01001110\n",
    "f: 01100110\n",
    "x: 01111000\n",
    "O: 01001111\n",
    "g: 01100111\n",
    "y: 01111001\n",
    "P: 01010000\n",
    "h: 01101000\n",
    "z: 01111010\n",
    "Q: 01010001\n",
    "i: 01101001\n",
    " \n",
    "R: 01010010\n",
    "j: 01101010\n",
    " \n",
    "\n",
    "You get the idea. So since we can express characters as 8-bit integers, we can express strings as arrays of 8-bit numbers characters.\n",
    "Three illustrations of the string \"NICE\": one in binary, one in base 10, and one in ASCII.\n",
    "Pointers\n",
    "\n",
    "Remember how we said every item in an array had to be the same size? Let's dig into that a little more.\n",
    "\n",
    "Suppose we wanted to store a bunch of ideas for baby names. Because we've got some really cute ones.\n",
    "\n",
    "Each name is a string. Which is really an array. And now we want to store those arrays in an array. Whoa.\n",
    "\n",
    "Now, what if our baby names have different lengths? That'd violate our rule that all the items in an array need to be the same size!\n",
    "\n",
    "We could put our baby names in arbitrarily large arrays (say, 13 characters each), and just use a special character to mark the end of the string within each array...\n",
    "Strings represented in RAM as arrays of 13 characters, with the end of the strings being denoted by a special \"null\" character. The last 8 rows are marked as wasted space because the name Bill (along with the null character) only takes up 5 out of 13 available characters.\n",
    "\n",
    "\"Wigglesworth\" is a cute baby name, right?\n",
    "\n",
    "But look at all that wasted space after \"Bill\". And what if we wanted to store a string that was more than 13 characters? We'd be out of luck.\n",
    "\n",
    "There's a better way. Instead of storing the strings right inside our array, let's just put the strings wherever we can fit them in memory. Then we'll have each element in our array hold the address in memory of its corresponding string. Each address is an integer, so really our outer array is just an array of integers. We can call each of these integers a pointer, since it points to another spot in memory.\n",
    "An array of names represented in RAM. The names are stored out of order, but an array holds the address in memory of each of name with arrows pointing from the number to the memory address.\n",
    "\n",
    "The pointers are marked with a * at the beginning.\n",
    "\n",
    "Pretty clever, right? This fixes both the disadvantages of arrays:\n",
    "\n",
    "    The items don't have to be the same length—each string can be as long or as short as we want.\n",
    "    We don't need enough uninterrupted free memory to store all our strings next to each other—we can place each of them separately, wherever there's space in RAM.\n",
    "\n",
    "We fixed it! No more tradeoffs. Right?\n",
    "\n",
    "Nope. Now we have a new tradeoff:\n",
    "\n",
    "Remember how the memory controller sends the contents of nearby memory addresses to the processor with each read? And the processor caches them? So reading sequential addresses in RAM is faster because we can get most of those reads right from the cache?\n",
    "A series of caches inside of the memory controller, where the processor stores what it has recently read from RAM.\n",
    "\n",
    "Our original array was very cache-friendly, because everything was sequential. So reading from the 0th index, then the 1st index, then the 2nd, etc. got an extra speedup from the processor cache.\n",
    "\n",
    "But the pointers in this array make it not cache-friendly, because the baby names are scattered randomly around RAM. So reading from the 0th index, then the 1st index, etc. doesn't get that extra speedup from the cache.\n",
    "\n",
    "That's the tradeoff. This pointer-based array requires less uninterrupted memory and can accommodate elements that aren't all the same size, but it's slower because it's not cache-friendly.\n",
    "\n",
    "This slowdown isn't reflected in the big O time cost. Lookups in this pointer-based array are still O(1)O(1)O(1) time.\n",
    "Dynamic arrays\n",
    "\n",
    "Let's build a very simple word processor. What data structure should we use to store the text as our user writes it?\n",
    "\n",
    "Strings are stored as arrays, right? So we should use an array?\n",
    "\n",
    "Here's where that gets tricky: when we allocate an array in a low-level language like C or Java, we have to specify upfront how many indices we want our array to have.\n",
    "\n",
    "There's a reason for this—the computer has to reserve space in memory for the array and commit to not letting anything else use that space. We can't have some other program overwriting the elements in our array!\n",
    "\n",
    "The computer can't reserve all its memory for a single array. So we have to tell it how much to reserve.\n",
    "\n",
    "But for our word processor, we don't know ahead of time how long the user's document is going to be! So what can we do?\n",
    "\n",
    "Just make an array and program it to resize itself when it runs out of space! This is called a dynamic array, and it's built on top of a normal array.\n",
    "\n",
    "Python, Ruby, and JavaScript use dynamic arrays for their default array-like data structures. In Python, they're called \"lists.\" Other languages have both. For example, in Java, array is a static array (whose size we have to define ahead of time) and ArrayList is a dynamic array.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "When you allocate a dynamic array, your dynamic array implementation makes an underlying static array. The starting size depends on the implementation—let's say our implementation uses 10 indices:\n",
    "A blank dynamic array created by default with 10 indices.\n",
    "\n",
    "Say you append 4 items to your dynamic array:\n",
    "The same dynamic array storing the word \"Dear.\"\n",
    "\n",
    "At this point, our dynamic array contains 4 items. It has a length of 4. But the underlying array has a length of 10.\n",
    "\n",
    "We'd say this dynamic array's size is 4 and its capacity is 10.\n",
    "Our dynamic array now has a size of 4 and a capacity of 10.\n",
    "\n",
    "The dynamic array stores an end_index to keep track of where the dynamic array ends and the extra capacity begins.\n",
    "The end_index of our dynamic array is marked at index 3, where the last letter of the word \"Dear\" is stored.\n",
    "\n",
    "If you keep appending, at some point you'll use up the full capacity of the underlying array:\n",
    "After adding 6 characters to form the string \"Dear Mothe,\" the end_index of our dynamic array is now marked at index 9, meaning the dynamic array is full.\n",
    "\n",
    "Next time you append, the dynamic array implementation will do a few things under the hood to make it work:\n",
    "\n",
    "1. Make a new, bigger array. Usually twice as big.\n",
    "\n",
    "Why not just extend the existing array? Because that memory might already be taken. Say we have Spotify open and it's using a handful of memory addresses right after the end of our old array. We'll have to skip that memory and reserve the next 20 uninterrupted memory slots for our new array:\n",
    "\n",
    "A new dynamic array, twice as big as the old dynamic array, is created in order to make more room.\n",
    "\n",
    "2. Copy each element from the old array into the new array.\n",
    "Each element from the old dynamic array is copied into the new dynamic array.\n",
    "\n",
    "3. Free up the old array. This tells the operating system, \"you can use this memory for something else now.\"\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
